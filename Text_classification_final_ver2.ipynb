{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-iPZFoQiasf"
      },
      "source": [
        "## Lab 2: Text Classification\n",
        "\n",
        "Note: For this lab exercise, it is recommended that you use [Google colab](https://colab.research.google.com/) to avoid issues concerning the deep learning module dependencies on your local system.\n",
        "\n",
        "For questions contact:\n",
        "\n",
        "Yash Pawar\n",
        "\n",
        "email ID: yash.pawar@dsv.su.se"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugaJVbFBraxp"
      },
      "source": [
        "## 1. Introduction\n",
        "\n",
        "\n",
        "In this lab exercise, we will perform classification of text into predefined classes using Machine Learning. In particular, we will be classifying the text from [BBC](http://mlg.ucd.ie/datasets/bbc.html) dataset consisting of 5 different classes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GU2tn5d_raxu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "## Suppress warnings\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE14c8aIraxv"
      },
      "source": [
        "### 2. Import Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ZjVIjeHXraxw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ],
      "source": [
        "# Note: The filepath has been specified considerning that the notebook is run using google colab.\n",
        "\n",
        "bbc = pd.read_csv(filepath_or_buffer='bbc_text.csv', delimiter = ',')\n",
        "bbc.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "082tyASTraxw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training_bbc:\n",
            "            category                                               text\n",
            "401            tech  progress on new internet domains by early 2005...\n",
            "311        business  lacroix label bought by us firm luxury goods g...\n",
            "2115           tech  cabs collect mountain of mobiles gadgets are c...\n",
            "20             tech  security warning over  fbi virus  the us feder...\n",
            "549   entertainment  berlin celebrates european cinema organisers s... \n",
            " 1668\n",
            "test_bbc:\n",
            "            category                                               text\n",
            "766   entertainment  applegate s charity show closes us musical swe...\n",
            "175        business  chinese exports rise 25% in 2004 exports from ...\n",
            "50            sport  lewsey puzzle over disallowed try england s jo...\n",
            "1388  entertainment  jungle tv show ratings drop by 4m the finale o...\n",
            "929            tech  casual gaming to  take off  games aimed at  ca...\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## To do: Replace the ??? with code to split the dataset into train and test set\n",
        "training_bbc, test_bbc = train_test_split(bbc)\n",
        "print(\"training_bbc:\\n\", training_bbc.head(),\"\\n\",len(training_bbc))\n",
        "print(\"test_bbc:\\n\", test_bbc.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUhRukvlraxx"
      },
      "source": [
        "### 3. Visualization\n",
        "\n",
        "Your task here is to get an understanding of distribution of different classes in the data by visualization and compare them.\n",
        "\n",
        "You are expected to generate two plots, on for each training and test dataset.\n",
        "\n",
        "You can refer to the [Bar plots tutorial](https://pythonguides.com/matplotlib-plot-bar-chart/) to know more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "S15Sb8iWraxx"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.axis.XTick at 0x1e4d1aa19d0>,\n",
              " <matplotlib.axis.XTick at 0x1e4d1aa19a0>,\n",
              " <matplotlib.axis.XTick at 0x1e4d1aa1640>,\n",
              " <matplotlib.axis.XTick at 0x1e4d1ad3a30>,\n",
              " <matplotlib.axis.XTick at 0x1e4d1ae11c0>]"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1152x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# To do: add the code below to plot the Distribution of classes in both the datasets.\n",
        "# Training data:\n",
        "fig = plt.figure(figsize=(16,4))\n",
        "ax = fig.add_subplot(1,2,1)\n",
        "\n",
        "ax.bar(training_bbc['category'].unique(), training_bbc['category'].value_counts(), color='deepskyblue')\n",
        "ax.set_title(\"Training data\")\n",
        "ax.set_xticks(training_bbc['category'].unique())\n",
        "\n",
        "# Test data:\n",
        "ax1 = fig.add_subplot(1,2,2)\n",
        "\n",
        "ax1.bar(test_bbc['category'].unique(), test_bbc['category'].value_counts(), color='violet')\n",
        "ax1.set_title(\"Test data\")\n",
        "ax1.set_xticks(test_bbc['category'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJeBNjcKrax1"
      },
      "source": [
        "### 5. Classification using Naive Bayes\n",
        "\n",
        "For training and validation, we will use a [Multinomial Naive Bayes classifier](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html). Here, you are expected to:\n",
        "\n",
        "1. Vectorize the text from the training set.\n",
        "2. Train the classifier\n",
        "3. Evaluate the classifier using the test set. \n",
        "\n",
        "Tip: You can use [sklearn's pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) functionality to perform steps 1 and 2. \n",
        "\n",
        "Tip: You can use [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to print the results of evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PEXADlvrax2"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Train and evaluate a Multinomial Naive Bayes classifier\n",
        "# To do: Add the code below to build a pipeline for the classifier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQR91FiQrax2"
      },
      "source": [
        "### 6. Baseline Classifier\n",
        "\n",
        "You can compare the performance of your Machine Learning model with a simple baseline classifier. One possibility could be to use a classifier that generates predictions by respecting the training setâ€™s class distribution. You can consider using [Dummy classifier](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html) from scikit learn.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZunaVeFrax3"
      },
      "outputs": [],
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "# Evaluate the random baseline\n",
        "baseline = DummyClassifier(strategy=\"stratified\")\n",
        "\n",
        "# To do: Add the code below to train the baseline classifier and evaluate it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hoa-P1QIrax4"
      },
      "source": [
        "Is the result from the baseline classifier justified?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwzvJ6qMrax4"
      },
      "source": [
        "### 6. Grid Search\n",
        "\n",
        "So far, you have trained the vectorizer and the classifer using their default parameters. However, in practical settings, one needs to optimize the parameters of the model to maximize the performance. \n",
        "\n",
        "Here, you are asked to find the optimal parameters for the pipelines that you have created above using a 5 fold cross validation. The choice of hyperparameters for optimization are:\n",
        "\n",
        "1. Bi-grams vs uni-grams vs tri-grams from [Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). \n",
        "2. Additive smoothing  for the Multinomial naive bayes classifier $\\alpha$ = {1, 0.1}\n",
        "3. Tokenized vs non-tokenized text (For tokenization, you can use the function 'preprocess' that is given below as a parameter for the vectorizer.)\n",
        "\n",
        "\n",
        "You can refer to the [Grid search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) documentation from the scikit-learn library.\n",
        "\n",
        "Finally, print the parameters from the grid search that give the best performance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdhGS2HMU3Ac"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function preprocess can be used as a tokenizer.\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm', disable=[\"tagger\", \"parser\", \"ner\"])\n",
        "\n",
        "def preprocess(text):\n",
        "    doc = nlp(text)\n",
        " \n",
        "    final_key=[]\n",
        "    for token in doc:\n",
        "        if token.is_stop==False and token.lemma_.isalpha():\n",
        "            \n",
        "            final_key.append(token.lemma_)\n",
        "        \n",
        "    return final_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOz9OcIlrax4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# To do: Replace the ??? in the code and implement the grid search\n",
        "# Note: Take a look at how you an specify the parameters for grid search from an example of n-grams. Similarly, you can specify the other remaining parameters.\n",
        "params = {'vectorizer__ngram_range':[(1,1), (1,2), (1,3)],\n",
        "          ???,\n",
        "          ???}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8ODDYpS626Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GISqsrKD8md0"
      },
      "source": [
        "## 7. Fine-tuning using BERT\n",
        "\n",
        "In this section, you will see how a pre-trained BERT model can be fine tuned for the task of text classification. \n",
        "\n",
        "Run the following cells to fine-tune the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-zjPdPiZLEG"
      },
      "outputs": [],
      "source": [
        "'Download the tokenizer and BERT module for python'\n",
        "\n",
        "#!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n",
        "\n",
        "\n",
        "\n",
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wks6VNo68xk-"
      },
      "outputs": [],
      "source": [
        "'Import all the necessary modules'\n",
        "\n",
        "#import tokenization\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import bert\n",
        "from tensorflow.keras.models import  Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"TensorFlow Version:\",tf.__version__)\n",
        "print(\"Hub version: \",hub.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCiSoIU28656"
      },
      "outputs": [],
      "source": [
        "'Download the pretrained BERT model'\n",
        "\n",
        "m_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
        "bert_layer = hub.KerasLayer(m_url, trainable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zK-Skk-K87oz"
      },
      "outputs": [],
      "source": [
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "#tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "\n",
        "'Use BERT tokenizer'\n",
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "tokenizer=FullTokenizer(vocab_file,do_lower_case)\n",
        "\n",
        "\n",
        "def bert_encode(texts, tokenizer, max_len=512):\n",
        "    all_tokens = []\n",
        "    all_masks = []\n",
        "    all_segments = []\n",
        "    \n",
        "    for text in texts:\n",
        "        text = tokenizer.tokenize(text)\n",
        "        \n",
        "        text = text[:max_len-2]\n",
        "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
        "        pad_len = max_len-len(input_sequence)\n",
        "        \n",
        "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n",
        "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
        "        segment_ids = [0] * max_len\n",
        "        \n",
        "        all_tokens.append(tokens)\n",
        "        all_masks.append(pad_masks)\n",
        "        all_segments.append(segment_ids)\n",
        "        \n",
        "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hF_gDNZn8-LP"
      },
      "outputs": [],
      "source": [
        "def build_model(bert_layer, max_len=512):\n",
        "    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
        "    \n",
        "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "    \n",
        "    clf_output = sequence_output[:, 0, :]\n",
        "    \n",
        "    lay = tf.keras.layers.Dense(64, activation='relu')(clf_output)\n",
        "    lay = tf.keras.layers.Dropout(0.2)(lay)\n",
        "    lay = tf.keras.layers.Dense(32, activation='relu')(lay)\n",
        "    lay = tf.keras.layers.Dropout(0.2)(lay)\n",
        "    out = tf.keras.layers.Dense(5, activation='softmax')(lay)\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
        "    model.compile(tf.keras.optimizers.Adam(lr=2e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eM05qXjS9Bbj"
      },
      "outputs": [],
      "source": [
        "'Set the maximum length of the sequence'\n",
        "max_len = 512\n",
        "\n",
        "'Transform non-numerical labels to numerical'\n",
        "label = preprocessing.LabelEncoder()\n",
        "train_labels = label.fit_transform(training_bbc['category'])\n",
        "train_labels = to_categorical(train_labels)\n",
        "\n",
        "'Prepare the input by tokenising and padding the text sequence'\n",
        "train_input = bert_encode(training_bbc.text.values, tokenizer, max_len=max_len)\n",
        "test_input = bert_encode(test_bbc.text.values, tokenizer, max_len=max_len)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0yJERe39F1F"
      },
      "outputs": [],
      "source": [
        "labels = label.classes_\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HudNWXyS9L5z"
      },
      "outputs": [],
      "source": [
        "'Build the model'\n",
        "\n",
        "model = build_model(bert_layer, max_len=max_len)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bxd085OY9Rn4"
      },
      "outputs": [],
      "source": [
        "'Start training the model'\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
        "\n",
        "train_sh = model.fit(\n",
        "    train_input, train_labels,\n",
        "    validation_split=0.1,\n",
        "    epochs=1,\n",
        "    callbacks=[checkpoint, earlystopping],\n",
        "    batch_size=4,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2Zr2lw1ClM7"
      },
      "outputs": [],
      "source": [
        "'Predict the classes from the fine-tuned BERT model'\n",
        "bert_pred = model.predict(test_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_E9ufij-DU2q"
      },
      "outputs": [],
      "source": [
        "'Invert the classes from numerical to non-numerical (original) categories'\n",
        "y_pred_bert = label.inverse_transform(np.argmax(bert_pred.round().astype(int), axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gnni25tiC-Ng"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(classification_report(test_bbc['category'], y_pred_bert, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyNKl7Z6_Sj5"
      },
      "source": [
        "1. Comment on the results. Is there any improvement in performance when compared to MultinomialNB?\n",
        "\n",
        "2. Try changing the number of epochs to 3 and then 5 to see if there is any improvement in the performance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLev7gYhF_D2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Text_classification_lab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
