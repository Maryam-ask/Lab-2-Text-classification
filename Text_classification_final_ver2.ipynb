{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-iPZFoQiasf"
      },
      "source": [
        "## Lab 2: Text Classification\n",
        "\n",
        "Note: For this lab exercise, it is recommended that you use [Google colab](https://colab.research.google.com/) to avoid issues concerning the deep learning module dependencies on your local system.\n",
        "\n",
        "For questions contact:\n",
        "\n",
        "Yash Pawar\n",
        "\n",
        "email ID: yash.pawar@dsv.su.se"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugaJVbFBraxp"
      },
      "source": [
        "## 1. Introduction\n",
        "\n",
        "\n",
        "In this lab exercise, we will perform classification of text into predefined classes using Machine Learning. In particular, we will be classifying the text from [BBC](http://mlg.ucd.ie/datasets/bbc.html) dataset consisting of 5 different classes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GU2tn5d_raxu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "## Suppress warnings\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE14c8aIraxv"
      },
      "source": [
        "### 2. Import Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ZjVIjeHXraxw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ],
      "source": [
        "# Note: The filepath has been specified considerning that the notebook is run using google colab.\n",
        "\n",
        "bbc = pd.read_csv(filepath_or_buffer='bbc_text.csv', delimiter = ',')\n",
        "bbc.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "082tyASTraxw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "715     entertainment\n",
            "422             sport\n",
            "235          business\n",
            "633     entertainment\n",
            "1163             tech\n",
            "            ...      \n",
            "898             sport\n",
            "548          politics\n",
            "1219         business\n",
            "811          business\n",
            "169     entertainment\n",
            "Name: category, Length: 557, dtype: object\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'print(\"training_bbc:\\n\", training_bbc.head(),\"\\n\",len(training_bbc))\\nprint(\"test_bbc:\\n\", test_bbc.head())'"
            ]
          },
          "execution_count": 152,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## To do: Replace the ??? with code to split the dataset into train and test set\n",
        "training_bbc, test_bbc = train_test_split(bbc)\n",
        "y_train = training_bbc['category']\n",
        "y_test = test_bbc['category']\n",
        "\"\"\"print(\"training_bbc:\\n\", training_bbc.head(),\"\\n\",len(training_bbc))\n",
        "print(\"test_bbc:\\n\", test_bbc.head())\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUhRukvlraxx"
      },
      "source": [
        "### 3. Visualization\n",
        "\n",
        "Your task here is to get an understanding of distribution of different classes in the data by visualization and compare them.\n",
        "\n",
        "You are expected to generate two plots, on for each training and test dataset.\n",
        "\n",
        "You can refer to the [Bar plots tutorial](https://pythonguides.com/matplotlib-plot-bar-chart/) to know more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "S15Sb8iWraxx"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.axis.XTick at 0x1e4d1c45820>,\n",
              " <matplotlib.axis.XTick at 0x1e4d1c514c0>,\n",
              " <matplotlib.axis.XTick at 0x1e4d1c5b640>,\n",
              " <matplotlib.axis.XTick at 0x1e4d1c9a8b0>,\n",
              " <matplotlib.axis.XTick at 0x1e4d1ca4100>]"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6UAAAEICAYAAACnPFJfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAlvklEQVR4nO3de5glVX3v//dHRkCFOAN0CM6Aww8n8RDzE0kHMeYkBNQAGof8DhqMR9GQZ2KCifeI8Sh4Es7BaET9mWDGgIzRoIgmcgjxSLiEaAAdlDsaJ1zCzBmcFgEvRCLwPX/s1bCnp6/Tl5re8349z366atWq2t9aXd2rvruq1k5VIUmSJElSFx7XdQCSJEmSpJ2XSakkSZIkqTMmpZIkSZKkzpiUSpIkSZI6Y1IqSZIkSeqMSakkSZIkqTMmpdI8SvL3SU6c67qzlaSSPG0h3kuSpJ1RkjuSPK/rOKTFwKRUGiPJ9/tejyT59775l89kW1V1TFWtm+u6CyXJypbALuk6FkmSpmsu+/K2vSuS/NZ8xNq274fF2ql5oimNUVV7jE4nuQP4rar6h7H1kiypqocWMjZJkjS16fblknYMXimVpinJEUk2JnlrkruBjyZZluSiJCNJ7m3TK/rWefST1SSvSvLFJO9tdW9Pcsx21j0wyZVJvpfkH5L8WZKPTxL7W5JsTvJ/kvzmmGUvTPK1JN9NcleS0/oWX9l+3tc+XX5OkoOSXJbkniTfTvKJJEtn0bSSJC2IJI9LckqSf2392PlJ9mrLdk/y8VZ+X5KvJNk3yenAfwY+1PrCD02w7VckubOt//Yxyw5LclXb7uYkH0qya1s22tde37b/61OdX0iDxqRUmpmfAPYCngqsofc39NE2fwDw78C4nVXzbOAbwD7AnwBnJ8l21P1r4MvA3sBpwCsmesMkRwNvBp4PrALGPt/yA+CVwFLghcDvJDmuLfvF9nNpVe1RVVcBAf4n8BTgPwH7txgkSdrR/R5wHPBL9Pqxe4E/a8tOBJ5Mr1/bG3gN8O9V9Xbgn4DXtr7wtWM3muRg4Cx6/fFT2vr9SeTDwBvo9enPAY4Cfhegqkb72me27X+KmZ9fSIuaSak0M48Ap1bVg1X171V1T1V9pqoeqKrvAafT6+gmcmdVfaSqHgbWAfsB+86kbpIDgJ8D3llV/1FVXwQunOQ9Xwp8tKpuqqofMCaBrKorqurGqnqkqm4AzptsH6pqQ1Vd0tpgBHjfFPssSdKO4jXA26tqY1U9SK9PPL6NnfAjesnk06rq4aq6tqq+O83tHg9cVFVXtu2+g945AwBtW1dX1UNVdQfwF0ze1870/EJa1HymVJqZkar64ehMkicCZwJHA8ta8Z5JdmnJ5Fh3j05U1QPtwuce49SbrO4+wHeq6oG+unfR+2R3PE8Bru2bv7N/YZJnA2cAzwB2BXYDPj3BtkiyL/ABercy7Unvw617J6ovSdIO5KnA3yR5pK/sYXofEP8Vvb70k+2xlI/TS2B/NI3tPoVeXwxAVf0gyT2j80l+kt6HuMPAE+mdg187diN99Wd6fiEtal4plWamxsy/Cfgp4NlV9WM8drvrRLfkzoXNwF6twxo1UUI6Wr9/+QFjlv81vSut+1fVk4EP81j8Y/cX4H+08p9p+/xfmd/9lSRprtwFHFNVS/teu1fVpqr6UVW9q6oOBn4eeBG9x1tg/P6w31Z9beuj9+5bfhbwdWBV6zv/kMn7zi7OL6TOmJRKs7Mnvec87msDJZw6329YVXcC64HTkuya5DnAr06yyvnAq5Ic3DrJsTHuSe/K6w+THAb8Rt+yEXq3H/0/Y+p/H7g/yXLgLbPbI0mSFsyHgdOTPBUgyVCS1W36l5P8TJJdgO/Su5139Irqt9i6LxzrAuBFSX6hDWD039n6PHvPts3vJ3k68Dtj1h+7/QU/v5C6ZFIqzc77gScA3wauBj6/QO/7cnoDJdwD/DHwKeDB8SpW1d/Ti/MyYEP72e93gf+e5HvAO+klsaPrPkDvOZYvtREDDwfeBRwK3A/8HfDZOdsrSZLm1wfo3R30hdbvXU1vYEHoDWZ4Ab3k8VbgH+nd0ju63vFtJNwPjt1oVd0MnEzv7qPN9B5r2dhX5c30PvT9HvARev12v9OAda2vfSndnV9InUjVVHcjSNrRJfkU8PWq8pNUSZIkLSpeKZUWoSQ/174v9HHtK19WA3/bcViSJEnSjDn6rrQ4/QS922b3pnd70O9U1de6DUmSJEmaOW/flSRJkiR1xtt3JUmSJEmd2SFu391nn31q5cqVXYchSRoQ11577beraqjrOBYz+2ZJ0lyarG/eIZLSlStXsn79+q7DkCQNiCR3dh3DYmffLEmaS5P1zd6+K0mSJEnqjEmpJEmLTJJzkmxJctM4y96UpJLs0+aT5INJNiS5IcmhCx+xJEkTMymVJGnxORc4emxhkv2BFwD/1ld8DLCqvdYAZy1AfJIkTZtJqSRJi0xVXQl8Z5xFZwJ/APR/39tq4GPVczWwNMl+CxCmJEnTYlIqSdIASLIa2FRV149ZtBy4q29+YysbbxtrkqxPsn5kZGSeIpUkaWvTTkqT7JLka0kuavMHJrmmPaPyqSS7tvLd2vyGtnzlPMUuSZKAJE8E/hB452y2U1Vrq2q4qoaHhvxGHUnSwpjJldLXAbf2zb8bOLOqngbcC5zUyk8C7m3lZ7Z6kiRp/hwEHAhcn+QOYAXw1SQ/AWwC9u+ru6KVSZK0Q5hWUppkBfBC4C/bfIAjgQtalXXAcW16dZunLT+q1ZckSfOgqm6sqh+vqpVVtZLeLbqHVtXdwIXAK9sovIcD91fV5i7jlSSp33SvlL6f3sAJj7T5vYH7quqhNt//fMqjz6605fe3+lvxuRVJkrZPkvOAq4CfSrIxyUmTVL8YuA3YAHwE+N0FCFGSpGlbMlWFJC8CtlTVtUmOmKs3rqq1wFqA4eHhmqL6tOWKudrS4lFHdB2BJGkhVdXLpli+sm+6gJPnO6bJ3PtH93b59p1Y9o5lXYcgSYvGlEkp8FzgxUmOBXYHfgz4AL0h5Ze0q6H9z6eMPruyMckS4MnAPXMeuSRJkiRp0Zvy9t2qeltVrWifup4AXFZVLwcuB45v1U4EPtemL2zztOWXtU9pJUmSJEnaymy+p/StwBuTbKD3zOjZrfxsYO9W/kbglNmFKEmSJEkaVNO5ffdRVXUFcEWbvg04bJw6PwReMgexSZIkSZIG3GyulEqSJEmSNCsmpZIkSZKkzpiUSpIkSZI6Y1IqSZIkSeqMSakkSZIkqTMmpZIkSZKkzpiUSpIkSZI6Y1IqSZIkSerMkq4DUPdyRdcRLLw6ousIJEmSJIFXSiVJkiRJHTIplSRJkiR1xqRUkiRJktQZk1JJkiRJUmdMSiVJkiRJnTEplSRJkiR1xqRUkiRJktQZk1JJkiRJUmemTEqT7J7ky0muT3Jzkne18nOT3J7kuvY6pJUnyQeTbEhyQ5JD53kfJEnaqSQ5J8mWJDf1lb0nyddb3/s3SZb2LXtb65e/keRXOglakqQJTOdK6YPAkVX1TOAQ4Ogkh7dlb6mqQ9rrulZ2DLCqvdYAZ81tyJIk7fTOBY4eU3YJ8Iyq+n+BfwHeBpDkYOAE4KfbOn+eZJeFC1WSpMlNmZRWz/fb7OPbqyZZZTXwsbbe1cDSJPvNPlRJkgRQVVcC3xlT9oWqeqjNXg2saNOrgU9W1YNVdTuwAThswYKVJGkK03qmNMkuSa4DtgCXVNU1bdHp7TahM5Ps1sqWA3f1rb6xlY3d5pok65OsHxkZ2f49kCRJY/0m8Pdtelr9Mtg3S5K6Ma2ktKoerqpD6H3qeliSZ9C7LejpwM8BewFvnckbV9XaqhququGhoaGZRS1JksaV5O3AQ8AnZrqufbMkqQszGn23qu4DLgeOrqrN7RbdB4GP8titQJuA/ftWW9HKJEnSPEryKuBFwMuravRRG/tlSdIObTqj7w6NjuCX5AnA84Gvjz4nmiTAccDoCIAXAq9so/AeDtxfVZvnIXZJktQkORr4A+DFVfVA36ILgROS7JbkQHoDEX65ixglSRrPkmnU2Q9Y10bqexxwflVdlOSyJENAgOuA17T6FwPH0htI4QHg1XMetSRJO7Ek5wFHAPsk2QicSu+xmt2AS3qfF3N1Vb2mqm5Ocj5wC73bek+uqoe7iVySpG1NmZRW1Q3As8YpP3KC+gWcPPvQJEnSeKrqZeMUnz1J/dOB0+cvIkmStt+MnimVJEmSJGkumZRKkiRJkjpjUipJkiRJ6oxJqSRJkiSpMyalkiRJkqTOmJRKkiRJkjpjUipJkiRJ6oxJqSRJkiSpM0u6DkCSJEmPufeP7u06hE4se8eyrkOQ1BGvlEqSJEmSOmNSKkmSJEnqjEmpJEmSJKkzJqWSJEmSpM440JEkSZIWNQeHkhY3k1JJkiRpJ2Mirx2JSak0Q7mi6wi6UUd0HYEkSZIGkc+USpIkSZI6M2VSmmT3JF9Ocn2Sm5O8q5UfmOSaJBuSfCrJrq18tza/oS1fOc/7IEmSJElapKZzpfRB4MiqeiZwCHB0ksOBdwNnVtXTgHuBk1r9k4B7W/mZrZ4kSZIkSduYMimtnu+32ce3VwFHAhe08nXAcW16dZunLT8qSeYqYEmSJEnS4JjWM6VJdklyHbAFuAT4V+C+qnqoVdkILG/Ty4G7ANry+4G9x9nmmiTrk6wfGRmZ1U5IkiRJkhanaSWlVfVwVR0CrAAOA54+2zeuqrVVNVxVw0NDQ7PdnCRJO40k5yTZkuSmvrK9klyS5Jvt57JWniQfbGM93JDk0O4ilyRpWzMafbeq7gMuB54DLE0y+pUyK4BNbXoTsD9AW/5k4J65CFaSJAFwLnD0mLJTgEurahVwaZsHOAZY1V5rgLMWKEZJkqZlyu8pTTIE/Kiq7kvyBOD59AYvuhw4HvgkcCLwubbKhW3+qrb8sqqqeYhd0iLhd7tKc6uqrhxndPvVwBFteh1wBfDWVv6x1hdfnWRpkv2qavMChStJ0qSmTEqB/YB1SXahd2X1/Kq6KMktwCeT/DHwNeDsVv9s4K+SbAC+A5wwD3FL0kAzkdd22Lcv0bwb2LdNPzrWQzM6DoRJqSRphzBlUlpVNwDPGqf8NnrPl44t/yHwkjmJTpIkzVhVVZIZ36WUZA29W3w54IAD5jwuSZLGM6NnSiVJ0g7rW0n2A2g/t7TyR8d6aPrHgdiKgxBKkrpgUipJ0mAYHdMBth3r4ZVtFN7Dgft9nlSStCOZzjOlkiRpB5LkPHqDGu2TZCNwKnAGcH6Sk4A7gZe26hcDxwIbgAeAVy94wJIkTcKkVJKkRaaqXjbBoqPGqVvAyfMbkSRJ28/bdyVJkiRJnTEplSRJkiR1xqRUkiRJktQZk1JJkiRJUmcc6EiSJEmSpnDvH93bdQidWPaOZfP+Hl4plSRJkiR1xqRUkiRJktQZk1JJkiRJUmdMSiVJkiRJnTEplSRJkiR1xqRUkiRJktQZk1JJkiRJUmdMSiVJkiRJnZkyKU2yf5LLk9yS5OYkr2vlpyXZlOS69jq2b523JdmQ5BtJfmU+d0CSJEmStHgtmUadh4A3VdVXk+wJXJvkkrbszKp6b3/lJAcDJwA/DTwF+IckP1lVD89l4JIkSZKkxW/KK6VVtbmqvtqmvwfcCiyfZJXVwCer6sGquh3YABw2F8FKkiRJkgbLjJ4pTbISeBZwTSt6bZIbkpyTZFkrWw7c1bfaRsZJYpOsSbI+yfqRkZGZRy5JkiRJWvSmnZQm2QP4DPD6qvoucBZwEHAIsBn405m8cVWtrarhqhoeGhqayaqSJEmSpAExraQ0yePpJaSfqKrPAlTVt6rq4ap6BPgIj92iuwnYv2/1Fa1MkiRJkqStTGf03QBnA7dW1fv6yvfrq/ZrwE1t+kLghCS7JTkQWAV8ee5CliRJkiQNiumMvvtc4BXAjUmua2V/CLwsySFAAXcAvw1QVTcnOR+4hd7IvSc78q4kSZIkaTxTJqVV9UUg4yy6eJJ1TgdOn0VckiRJkqSdwIxG35UkSTu2JG9IcnOSm5Kcl2T3JAcmuSbJhiSfSrJr13FKkjTKpFSSpAGRZDnw+8BwVT0D2AU4AXg3cGZVPQ24FzipuyglSdqaSakkSYNlCfCEJEuAJ9L72rYjgQva8nXAcd2EJknStkxKJUkaEFW1CXgv8G/0ktH7gWuB+6rqoVZtI7B8vPWTrEmyPsn6kZGRhQhZkiSTUkmSBkWSZcBq4EDgKcCTgKOnu35Vra2q4aoaHhoamqcoJUnamkmpJEmD43nA7VU1UlU/Aj5L76vdlrbbeQFWAJu6ClCSpLFMSiVJGhz/Bhye5IlJAhxF73vDLweOb3VOBD7XUXySJG3DpFSSpAFRVdfQG9Doq8CN9Pr5tcBbgTcm2QDsDZzdWZCSJI2xZOoqkiRpsaiqU4FTxxTfBhzWQTiSJE3JK6WSJEmSpM6YlEqSJEmSOmNSKkmSJEnqjEmpJEmSJKkzJqWSJEmSpM6YlEqSJEmSOmNSKkmSJEnqjEmpJEmSJKkzUyalSfZPcnmSW5LcnOR1rXyvJJck+Wb7uayVJ8kHk2xIckOSQ+d7JyRJkiRJi9N0rpQ+BLypqg4GDgdOTnIwcApwaVWtAi5t8wDHAKvaaw1w1pxHLUmSJEkaCFMmpVW1uaq+2qa/B9wKLAdWA+tatXXAcW16NfCx6rkaWJpkv7kOXJIkSZK0+M3omdIkK4FnAdcA+1bV5rbobmDfNr0cuKtvtY2tbOy21iRZn2T9yMjITOOWJEmSJA2AaSelSfYAPgO8vqq+27+sqgqombxxVa2tquGqGh4aGprJqpIkSZKkATGtpDTJ4+klpJ+oqs+24m+N3pbbfm5p5ZuA/ftWX9HKJEmSJEnaynRG3w1wNnBrVb2vb9GFwIlt+kTgc33lr2yj8B4O3N93m68kSZIkSY9aMo06zwVeAdyY5LpW9ofAGcD5SU4C7gRe2pZdDBwLbAAeAF49lwFLkiRJkgbHlElpVX0RyASLjxqnfgEnzzIuSZIkSdJOYEaj70qSJEmSNJdMSiVJkiRJnTEplSRJkiR1xqRUkiRJktQZk1JJkgZIkqVJLkjy9SS3JnlOkr2SXJLkm+3nsq7jlCRplEmpJEmD5QPA56vq6cAzgVuBU4BLq2oVcGmblyRph2BSKknSgEjyZOAXgbMBquo/quo+YDWwrlVbBxzXRXySJI3HpFSSpMFxIDACfDTJ15L8ZZInAftW1eZW525g3/FWTrImyfok60dGRhYoZEnSzs6kVJKkwbEEOBQ4q6qeBfyAMbfqVlUBNd7KVbW2qoaranhoaGjeg5UkCUxKJUkaJBuBjVV1TZu/gF6S+q0k+wG0n1s6ik+SpG2YlEqSNCCq6m7griQ/1YqOAm4BLgRObGUnAp/rIDxJksa1pOsAJEnSnPo94BNJdgVuA15N70Po85OcBNwJvLTD+CRJ2opJqSRJA6SqrgOGx1l01AKHIknStHj7riRJkiSpMyalkiRJkqTOmJRKkiRJkjpjUipJkiRJ6syUSWmSc5JsSXJTX9lpSTYlua69ju1b9rYkG5J8I8mvzFfgkiRJkqTFbzpXSs8Fjh6n/MyqOqS9LgZIcjBwAvDTbZ0/T7LLXAUrSZIkSRosUyalVXUl8J1pbm818MmqerCqbgc2AIfNIj5JkiRJ0gCbzTOlr01yQ7u9d1krWw7c1VdnYyvbRpI1SdYnWT8yMjKLMCRJkiRJi9X2JqVnAQcBhwCbgT+d6Qaqam1VDVfV8NDQ0HaGIUmSJElazLYrKa2qb1XVw1X1CPARHrtFdxOwf1/VFa1MkiRJkqRtbFdSmmS/vtlfA0ZH5r0QOCHJbkkOBFYBX55diJIkSZKkQbVkqgpJzgOOAPZJshE4FTgiySFAAXcAvw1QVTcnOR+4BXgIOLmqHp6XyCVJkiRJi96USWlVvWyc4rMnqX86cPpsgpIkSZIk7RxmM/quJEmSJEmzYlIqSZIkSeqMSakkSZIkqTMmpZIkSZKkzpiUSpIkSZI6Y1IqSZIkSeqMSakkSZIkqTMmpZIkSZKkzpiUSpI0YJLskuRrSS5q8wcmuSbJhiSfSrJr1zFKkjTKpFSSpMHzOuDWvvl3A2dW1dOAe4GTOolKkqRxmJRKkjRAkqwAXgj8ZZsPcCRwQauyDjiuk+AkSRqHSakkSYPl/cAfAI+0+b2B+6rqoTa/EVg+3opJ1iRZn2T9yMjIvAcqSRKYlEqSNDCSvAjYUlXXbs/6VbW2qoaranhoaGiOo5MkaXxLug5AkiTNmecCL05yLLA78GPAB4ClSZa0q6UrgE0dxihJ0la8UipJ0oCoqrdV1YqqWgmcAFxWVS8HLgeOb9VOBD7XUYiSJG3DpFSSpMH3VuCNSTbQe8b07I7jkSTpUd6+K0nSAKqqK4Ar2vRtwGFdxiNJ0kSmvFKa5JwkW5Lc1Fe2V5JLknyz/VzWypPkg+3LuW9Icuh8Bi9JkiRJWtymc/vuucDRY8pOAS6tqlXApW0e4BhgVXutAc6amzAlSZIkSYNoyqS0qq4EvjOmeDW9L9+Grb+EezXwseq5mt5of/vNUaySJEmSpAGzvQMd7VtVm9v03cC+bXo5cFdfPb+gW5IkSZI0oVmPvltVBdR2rOcXdEuSJEnSTm57k9Jvjd6W235uaeWbgP376vkF3ZIkSZKkCW1vUnohvS/fhq2/hPtC4JVtFN7Dgfv7bvOVJEmSJGkrU35PaZLzgCOAfZJsBE4FzgDOT3IScCfw0lb9YuBYYAPwAPDqeYhZkiRJkjQgpkxKq+plEyw6apy6BZw826AkSZIkSTuHWQ90JEmSJEnS9jIplSRJkiR1xqRUkiRJktQZk1JJkiRJUmdMSiVJkiRJnTEplSRJkiR1xqRUkiRJktQZk1JJkiRJUmdMSiVJkiRJnTEplSRJkiR1xqRUkiRJktQZk1JJkiRJUmdMSiVJGhBJ9k9yeZJbktyc5HWtfK8klyT5Zvu5rOtYJUkaZVIqSdLgeAh4U1UdDBwOnJzkYOAU4NKqWgVc2uYlSdohmJRKkjQgqmpzVX21TX8PuBVYDqwG1rVq64DjOglQkqRxmJRKkjSAkqwEngVcA+xbVZvboruBfbuKS5KksUxKJUkaMEn2AD4DvL6qvtu/rKoKqAnWW5NkfZL1IyMjCxCpJEmzTEqT3JHkxiTXJVnfyhxMQZKkjiR5PL2E9BNV9dlW/K0k+7Xl+wFbxlu3qtZW1XBVDQ8NDS1MwJKknd5cXCn95ao6pKqG27yDKUiS1IEkAc4Gbq2q9/UtuhA4sU2fCHxuoWOTJGki83H7roMpSJLUjecCrwCObHcxXZfkWOAM4PlJvgk8r81LkrRDWDLL9Qv4QpIC/qKq1jLNwRSSrAHWABxwwAGzDEOSJFXVF4FMsPiohYxFkqTpmm1S+gtVtSnJjwOXJPl6/8KqqpawbqMlsGsBhoeHx60jSZIkSRpss7p9t6o2tZ9bgL8BDmOagylIkiRJkrTdSWmSJyXZc3QaeAFwEw6mIEmSJEmaptncvrsv8De9gf5YAvx1VX0+yVeA85OcBNwJvHT2YUqSJEmSBtF2J6VVdRvwzHHK78HBFCRJkiRJ0zAfXwkjSZIkSdK0mJRKkiRJkjpjUipJkiRJ6oxJqSRJkiSpMyalkiRJkqTOmJRKkiRJkjpjUipJkiRJ6oxJqSRJkiSpMyalkiRJkqTOmJRKkiRJkjpjUipJkiRJ6oxJqSRJkiSpMyalkiRJkqTOmJRKkiRJkjpjUipJkiRJ6oxJqSRJkiSpMyalkiRJkqTOzFtSmuToJN9IsiHJKfP1PpIkaXrsmyVJO6J5SUqT7AL8GXAMcDDwsiQHz8d7SZKkqdk3S5J2VPN1pfQwYENV3VZV/wF8Elg9T+8lSZKmZt8sSdohLZmn7S4H7uqb3wg8u79CkjXAmjb7/STfmKdYFso+wLe7eON08aZzwzabOdts5myzmRuENnvq3G1qYNg3L6R3dvKuc8E2mznbbOZss5kbhDabsG+er6R0SlW1Fljb1fvPtSTrq2q46zgWE9ts5myzmbPNZs4223nZN8s2mznbbOZss5kb9Dabr9t3NwH7982vaGWSJKkb9s2SpB3SfCWlXwFWJTkwya7ACcCF8/RekiRpavbNkqQd0rzcvltVDyV5LfC/gV2Ac6rq5vl4rx3IwNzutIBss5mzzWbONps522wA2TdrmmyzmbPNZs42m7mBbrNUVdcxSJIkSZJ2UvN1+64kSZIkSVMyKZUkSZIkdcakdApJlib53e1c99wkx891TDuTJEck+fmu4+hCkiuSDLfpi9uxuNXxmOQpSS7oLsq5lWRlkptmuY2BapPxJDkuycHbsd60/p6SvDjJKdsX3ezM5n+uFieP58VnMffN9jPzx3PmubMzngOalE5tKTBwHcpikGQJcASwKDu+uVRVx1bVfYw5Hqvq/1SV/8T77CRtchwwo5P4mfw9VdWFVXXGdkU2e0vxf+7O5jg8nhcN++adpp/ZHksZsON9R7CznAOalE7tDOCgJNcleU+StyT5SpIbkrxrtFKSV7ay65P8Vd/6v5jkn5PcNuifACV5UpK/a21wU5JfT3JHkj9JcmOSLyd5Wqu7Msllrc0uTXJAKz83yYeTXAOcD7wGeENr///c4e7NWtvnryf5RJJbk1yQ5IlJjkrytdZG5yTZbZx170iyD9sej49+4ptklyTvbW1/Q5Lfa+VnJLmllb13Yfd6uywZp41G958kw0muaNO/1NriutaGe45pk1cl+WySzyf5ZpI/GX2TJC9IclWSryb5dJI9Wvk27ZXkJa1dr09y5XzsdJL/2v5GrkvyF+33+f0kp7f3vTrJvu3qxIuB97S6B7XX55Ncm+Sfkjy9bXPSv6ckv5rkmtZ2/5Bk3752+1DfNj449v9YeldK/jHJ51r5GUle3vbhxiQHtXpDST6T3v/NryR5bis/rR3vV7T1f781xVbH+Hy0teafx/OOczzHvnk8O2U/swA8Z55APAecWlX5muQFrARuatMvoDccc+gl9BcBvwj8NPAvwD6t3l7t57nAp1vdg4ENXe/PPLfVfwE+0jf/ZOAO4O1t/pXARW36fwEntunfBP62r80uAnZp86cBb+563+bwWCrguW3+HOC/AXcBP9nKPga8vk1fAQy36TuAffqPx3GOz98BLgCWjB6HwN7AN3hspO2lXbfDdrTRm0f3v5UNA1f0HUejdfeg9zVX/W3yKuC2dizuDtwJ7N/a8krgSa3eW4F3TtRewI3A8vlqQ+A/tX15fJv/8/b3UsCvtrI/Af5bmz4XOL5v/UuBVW362cBlffUm/HsClvXt628Bf9rXbh/q28Y2/8foXSm5D9gP2A3YBLyrLXsd8P42/dfAL7TpA4Bb+2L557buPsA9wOMZc4z7Wnwvj+cd63jGvnlse6xkJ+xnFrBtPWee/nHnOWDfa16+p3SAvaC9vtbm9wBWAc8EPl1V3waoqu/0rfO3VfUIcMvop7YD7EbgT5O8m14H909JAM5ry88DzmzTzwH+vzb9V/ROUEZ9uqoeXoB4u3BXVX2pTX8ceAdwe1X9SytbB5wMvH87tv084MNV9RD0jsP0brP6IXB2kovodQo7urFt9PuT1P0S8L4knwA+W1Ub2zHX79Kquh8gyS3AU+ndAnMw8KVWf1fgKuB+xm+vLwHnJjkf+Ozsdm9cRwE/C3ylxfMEYAvwH30xXAs8f+yK7ZP3nwc+3bfv/Z+0Tvb3tAL4VJL96LXB7RPUm+j/2FeqanOL41+BL7TyG4FfbtPPAw7ui+3HRq8WAH9XVQ8CDybZAgz6/8idhcfzjnU82zdva2fsZxaa58zb8hxwEialMxPgf1bVX2xV2C6RT+DBMesPrKr6lySHAscCf5zk0tFF/dWmsakfzHlwO46x+38fvU+y5ufNqh5Kchi9k8TjgdcCR87X+82RsW1UwEM89rjB7o8uqDojyd/RO+a+lORX6P0D7tf/N/gwvf97AS6pqpeNffPx2quqXpPk2cALgWuT/GxV3bO9OziOAOuq6m1jYnlztY83+2If63HAfVV1yATbnuzv6f8H3ldVFyY5gt7Vj/FM9H+sv/yRvvlH+mJ9HHB4VW31e2knaeP9brT4eTzvQMezffO4dsZ+ZqF5zrwtzwEn4TOlU/sesGeb/t/Ab/Y9E7A8yY8DlwEvSbJ3K9+rk0g7luQpwANV9XHgPcChbdGv9/28qk3/M3BCm3458E8TbLa//QfBAUme06Z/A1gPrEx7ngd4BfCPk6w/WXtcAvx2+2SMJHu1Y/XJVXUx8AZ6n1Du6Ma20Rfp3brys63sv4xWTHJQVd1YVe8GvgI8fZrvcTXw3Dz2HNWTkvzkRO3V3ueaqnonMELv1qy5dClwfPt/Mvq7e+ok9R89Dqrqu8DtSV7S1k2SiX7PY4+fJ9O7TRHgxFnEP5kvAI+ehCQ5ZIr6g/Y3vzPyeH5M58ezffO4dsZ+ZiF4zjw5zwEnYVI6hfYp1ZfSe5D4+fSeJ7kqyY307t3es6puBk4H/jHJ9cD7Ogu4Wz8DfDnJdcCpwB+38mVJbqD3XM4bWtnvAa9u5a9oy8bzv4Bfy+IdTGGsbwAnJ7mV3vNPZwKvpner2o30PpH/8EQr9x+P2XbQjL8E/g24oR2Hv0Hvn9dFrZ2/CLxxrndoHoxto7OAdwEfSLKe3qfQo17f2uIG4EfA30/nDapqhN5zQOe1da+id6IxUXu9J71BCG6id9J2/Sz3cWw8t9B7tuQL7b0vofds20Q+CbwlvcERDqJ38nhS+73fDKyeYL2xf0+n0Tv2rgW+PTd7s43fB4bTG2ThFnoDpExoimNci4DH82N2kOPZvnlbO10/sxA8Z56S54CTGH3wVZoXSe6g96D2fJ0gLBpJVtJ7nucZXcciSdp52TdLC8tzwKl5pVSSJEmS1BmvlEqSJEmSOuOVUkmSJElSZ0xKJUmSJEmdMSmVJEmSJHXGpFSSJEmS1BmTUkmSJElSZ/4v8WN4HB6SC+kAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1152x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# To do: add the code below to plot the Distribution of classes in both the datasets.\n",
        "# Training data:\n",
        "fig = plt.figure(figsize=(16,4))\n",
        "ax = fig.add_subplot(1,2,1)\n",
        "\n",
        "ax.bar(y_train.unique(), y_train.value_counts(), color='deepskyblue')\n",
        "ax.set_title(\"Training data\")\n",
        "ax.set_xticks(y_train.unique())\n",
        "\n",
        "# Test data:\n",
        "ax1 = fig.add_subplot(1,2,2)\n",
        "\n",
        "ax1.bar(y_test.unique(), y_test.value_counts(), color='violet')\n",
        "ax1.set_title(\"Test data\")\n",
        "ax1.set_xticks(y_test.unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJeBNjcKrax1"
      },
      "source": [
        "### 5. Classification using Naive Bayes\n",
        "\n",
        "For training and validation, we will use a [Multinomial Naive Bayes classifier](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html). Here, you are expected to:\n",
        "\n",
        "1. Vectorize the text from the training set.\n",
        "2. Train the classifier\n",
        "3. Evaluate the classifier using the test set. \n",
        "\n",
        "Tip: You can use [sklearn's pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) functionality to perform steps 1 and 2. \n",
        "\n",
        "Tip: You can use [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to print the results of evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "2PEXADlvrax2"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "EOL while scanning string literal (Temp/ipykernel_23892/1380586160.py, line 20)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_23892/1380586160.py\"\u001b[1;36m, line \u001b[1;32m20\u001b[0m\n\u001b[1;33m    print(classification_report(test_bbc, predict_bbc))\"\"\"\"\u001b[0m\n\u001b[1;37m                                                           \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# Train and evaluate a Multinomial Naive Bayes classifier\n",
        "# To do: Add the code below to build a pipeline for the classifier.\n",
        "# count Vectorizer:\n",
        "\"\"\"\"cv = CountVectorizer()\n",
        "cv.fit_transform(training_bbc)\n",
        "\n",
        "# Naive Baies:\n",
        "nb = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', MultinomialNB()),\n",
        "              ])\n",
        "nb.fit(training_bbc)\n",
        "predict_bbc = nb.predict(test_bbc)\n",
        "print(classification_report(test_bbc, predict_bbc))\"\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQR91FiQrax2"
      },
      "source": [
        "### 6. Baseline Classifier\n",
        "\n",
        "You can compare the performance of your Machine Learning model with a simple baseline classifier. One possibility could be to use a classifier that generates predictions by respecting the training setâ€™s class distribution. You can consider using [Dummy classifier](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html) from scikit learn.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZunaVeFrax3"
      },
      "outputs": [],
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "# Evaluate the random baseline\n",
        "baseline = DummyClassifier(strategy=\"stratified\")\n",
        "\n",
        "# To do: Add the code below to train the baseline classifier and evaluate it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hoa-P1QIrax4"
      },
      "source": [
        "Is the result from the baseline classifier justified?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwzvJ6qMrax4"
      },
      "source": [
        "### 6. Grid Search\n",
        "\n",
        "So far, you have trained the vectorizer and the classifer using their default parameters. However, in practical settings, one needs to optimize the parameters of the model to maximize the performance. \n",
        "\n",
        "Here, you are asked to find the optimal parameters for the pipelines that you have created above using a 5 fold cross validation. The choice of hyperparameters for optimization are:\n",
        "\n",
        "1. Bi-grams vs uni-grams vs tri-grams from [Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). \n",
        "2. Additive smoothing  for the Multinomial naive bayes classifier $\\alpha$ = {1, 0.1}\n",
        "3. Tokenized vs non-tokenized text (For tokenization, you can use the function 'preprocess' that is given below as a parameter for the vectorizer.)\n",
        "\n",
        "\n",
        "You can refer to the [Grid search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) documentation from the scikit-learn library.\n",
        "\n",
        "Finally, print the parameters from the grid search that give the best performance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdhGS2HMU3Ac"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function preprocess can be used as a tokenizer.\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm', disable=[\"tagger\", \"parser\", \"ner\"])\n",
        "\n",
        "def preprocess(text):\n",
        "    doc = nlp(text)\n",
        " \n",
        "    final_key=[]\n",
        "    for token in doc:\n",
        "        if token.is_stop==False and token.lemma_.isalpha():\n",
        "            \n",
        "            final_key.append(token.lemma_)\n",
        "        \n",
        "    return final_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOz9OcIlrax4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# To do: Replace the ??? in the code and implement the grid search\n",
        "# Note: Take a look at how you an specify the parameters for grid search from an example of n-grams. Similarly, you can specify the other remaining parameters.\n",
        "params = {'vectorizer__ngram_range':[(1,1), (1,2), (1,3)],\n",
        "          ???,\n",
        "          ???}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8ODDYpS626Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GISqsrKD8md0"
      },
      "source": [
        "## 7. Fine-tuning using BERT\n",
        "\n",
        "In this section, you will see how a pre-trained BERT model can be fine tuned for the task of text classification. \n",
        "\n",
        "Run the following cells to fine-tune the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-zjPdPiZLEG"
      },
      "outputs": [],
      "source": [
        "'Download the tokenizer and BERT module for python'\n",
        "\n",
        "#!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n",
        "\n",
        "\n",
        "\n",
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wks6VNo68xk-"
      },
      "outputs": [],
      "source": [
        "'Import all the necessary modules'\n",
        "\n",
        "#import tokenization\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import bert\n",
        "from tensorflow.keras.models import  Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"TensorFlow Version:\",tf.__version__)\n",
        "print(\"Hub version: \",hub.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCiSoIU28656"
      },
      "outputs": [],
      "source": [
        "'Download the pretrained BERT model'\n",
        "\n",
        "m_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
        "bert_layer = hub.KerasLayer(m_url, trainable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zK-Skk-K87oz"
      },
      "outputs": [],
      "source": [
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "#tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "\n",
        "'Use BERT tokenizer'\n",
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "tokenizer=FullTokenizer(vocab_file,do_lower_case)\n",
        "\n",
        "\n",
        "def bert_encode(texts, tokenizer, max_len=512):\n",
        "    all_tokens = []\n",
        "    all_masks = []\n",
        "    all_segments = []\n",
        "    \n",
        "    for text in texts:\n",
        "        text = tokenizer.tokenize(text)\n",
        "        \n",
        "        text = text[:max_len-2]\n",
        "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
        "        pad_len = max_len-len(input_sequence)\n",
        "        \n",
        "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n",
        "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
        "        segment_ids = [0] * max_len\n",
        "        \n",
        "        all_tokens.append(tokens)\n",
        "        all_masks.append(pad_masks)\n",
        "        all_segments.append(segment_ids)\n",
        "        \n",
        "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hF_gDNZn8-LP"
      },
      "outputs": [],
      "source": [
        "def build_model(bert_layer, max_len=512):\n",
        "    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
        "    \n",
        "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "    \n",
        "    clf_output = sequence_output[:, 0, :]\n",
        "    \n",
        "    lay = tf.keras.layers.Dense(64, activation='relu')(clf_output)\n",
        "    lay = tf.keras.layers.Dropout(0.2)(lay)\n",
        "    lay = tf.keras.layers.Dense(32, activation='relu')(lay)\n",
        "    lay = tf.keras.layers.Dropout(0.2)(lay)\n",
        "    out = tf.keras.layers.Dense(5, activation='softmax')(lay)\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
        "    model.compile(tf.keras.optimizers.Adam(lr=2e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eM05qXjS9Bbj"
      },
      "outputs": [],
      "source": [
        "'Set the maximum length of the sequence'\n",
        "max_len = 512\n",
        "\n",
        "'Transform non-numerical labels to numerical'\n",
        "label = preprocessing.LabelEncoder()\n",
        "train_labels = label.fit_transform(training_bbc['category'])\n",
        "train_labels = to_categorical(train_labels)\n",
        "\n",
        "'Prepare the input by tokenising and padding the text sequence'\n",
        "train_input = bert_encode(training_bbc.text.values, tokenizer, max_len=max_len)\n",
        "test_input = bert_encode(test_bbc.text.values, tokenizer, max_len=max_len)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0yJERe39F1F"
      },
      "outputs": [],
      "source": [
        "labels = label.classes_\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HudNWXyS9L5z"
      },
      "outputs": [],
      "source": [
        "'Build the model'\n",
        "\n",
        "model = build_model(bert_layer, max_len=max_len)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bxd085OY9Rn4"
      },
      "outputs": [],
      "source": [
        "'Start training the model'\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
        "\n",
        "train_sh = model.fit(\n",
        "    train_input, train_labels,\n",
        "    validation_split=0.1,\n",
        "    epochs=1,\n",
        "    callbacks=[checkpoint, earlystopping],\n",
        "    batch_size=4,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2Zr2lw1ClM7"
      },
      "outputs": [],
      "source": [
        "'Predict the classes from the fine-tuned BERT model'\n",
        "bert_pred = model.predict(test_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_E9ufij-DU2q"
      },
      "outputs": [],
      "source": [
        "'Invert the classes from numerical to non-numerical (original) categories'\n",
        "y_pred_bert = label.inverse_transform(np.argmax(bert_pred.round().astype(int), axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gnni25tiC-Ng"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(classification_report(test_bbc['category'], y_pred_bert, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyNKl7Z6_Sj5"
      },
      "source": [
        "1. Comment on the results. Is there any improvement in performance when compared to MultinomialNB?\n",
        "\n",
        "2. Try changing the number of epochs to 3 and then 5 to see if there is any improvement in the performance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLev7gYhF_D2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Text_classification_lab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
