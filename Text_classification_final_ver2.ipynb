{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-iPZFoQiasf"
      },
      "source": [
        "## Lab 2: Text Classification\n",
        "\n",
        "Note: For this lab exercise, it is recommended that you use [Google colab](https://colab.research.google.com/) to avoid issues concerning the deep learning module dependencies on your local system.\n",
        "\n",
        "For questions contact:\n",
        "\n",
        "Yash Pawar\n",
        "\n",
        "email ID: yash.pawar@dsv.su.se"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugaJVbFBraxp"
      },
      "source": [
        "## 1. Introduction\n",
        "\n",
        "\n",
        "In this lab exercise, we will perform classification of text into predefined classes using Machine Learning. In particular, we will be classifying the text from [BBC](http://mlg.ucd.ie/datasets/bbc.html) dataset consisting of 5 different classes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GU2tn5d_raxu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "## Suppress warnings\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE14c8aIraxv"
      },
      "source": [
        "### 2. Import Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZjVIjeHXraxw"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tech</td>\n",
              "      <td>tv future in the hands of viewers with home th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>business</td>\n",
              "      <td>worldcom boss  left books alone  former worldc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sport</td>\n",
              "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sport</td>\n",
              "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        category                                               text\n",
              "0           tech  tv future in the hands of viewers with home th...\n",
              "1       business  worldcom boss  left books alone  former worldc...\n",
              "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
              "3          sport  yeading face newcastle in fa cup premiership s...\n",
              "4  entertainment  ocean s twelve raids box office ocean s twelve..."
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Note: The filepath has been specified considerning that the notebook is run using google colab.\n",
        "\n",
        "bbc = pd.read_csv(filepath_or_buffer='bbc_text.csv', delimiter = ',')\n",
        "bbc.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "082tyASTraxw"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'print(\"training_bbc:\\n\", training_bbc.head(),\"\\n\",len(training_bbc))\\nprint(\"test_bbc:\\n\", test_bbc.head())'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## To do: Replace the ??? with code to split the dataset into train and test set\n",
        "training_bbc, test_bbc = train_test_split(bbc)\n",
        "\"\"\"print(\"training_bbc:\\n\", training_bbc.head(),\"\\n\",len(training_bbc))\n",
        "print(\"test_bbc:\\n\", test_bbc.head())\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUhRukvlraxx"
      },
      "source": [
        "### 3. Visualization\n",
        "\n",
        "Your task here is to get an understanding of distribution of different classes in the data by visualization and compare them.\n",
        "\n",
        "You are expected to generate two plots, on for each training and test dataset.\n",
        "\n",
        "You can refer to the [Bar plots tutorial](https://pythonguides.com/matplotlib-plot-bar-chart/) to know more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "S15Sb8iWraxx"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.axis.XTick at 0x2567ac27d30>,\n",
              " <matplotlib.axis.XTick at 0x2567ac27c40>,\n",
              " <matplotlib.axis.XTick at 0x2567ac2cf40>,\n",
              " <matplotlib.axis.XTick at 0x2567aa58160>,\n",
              " <matplotlib.axis.XTick at 0x2567aa588b0>]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1152x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# To do: add the code below to plot the Distribution of classes in both the datasets.\n",
        "# Training data:\n",
        "fig = plt.figure(figsize=(16,4))\n",
        "ax = fig.add_subplot(1,2,1)\n",
        "\n",
        "ax.bar(training_bbc['category'].unique(), training_bbc['category'].value_counts(), color='deepskyblue')\n",
        "ax.set_title(\"Training data\")\n",
        "ax.set_xticks(training_bbc['category'].unique())\n",
        "\n",
        "# Test data:\n",
        "ax1 = fig.add_subplot(1,2,2)\n",
        "\n",
        "ax1.bar(test_bbc['category'].unique(), test_bbc['category'].value_counts(), color='violet')\n",
        "ax1.set_title(\"Test data\")\n",
        "ax1.set_xticks(test_bbc['category'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJeBNjcKrax1"
      },
      "source": [
        "### 5. Classification using Naive Bayes\n",
        "\n",
        "For training and validation, we will use a [Multinomial Naive Bayes classifier](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html). Here, you are expected to:\n",
        "\n",
        "1. Vectorize the text from the training set.\n",
        "2. Train the classifier\n",
        "3. Evaluate the classifier using the test set. \n",
        "\n",
        "Tip: You can use [sklearn's pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) functionality to perform steps 1 and 2. \n",
        "\n",
        "Tip: You can use [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to print the results of evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "2PEXADlvrax2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business       0.98      0.96      0.97       137\n",
            "entertainment       0.98      0.93      0.95        97\n",
            "     politics       0.93      0.97      0.95       118\n",
            "        sport       1.00      1.00      1.00       121\n",
            "         tech       0.93      0.96      0.95        84\n",
            "\n",
            "     accuracy                           0.97       557\n",
            "    macro avg       0.96      0.96      0.96       557\n",
            " weighted avg       0.97      0.97      0.97       557\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# Train and evaluate a Multinomial Naive Bayes classifier\n",
        "# To do: Add the code below to build a pipeline for the classifier.\n",
        "# count Vectorizer:\n",
        "count_vectorize_bbc = CountVectorizer()\n",
        "count_vectorize_bbc.fit_transform(training_bbc['text'])\n",
        "features = count_vectorize_bbc.get_feature_names()\n",
        "#print(features)\n",
        "\n",
        "# Naive Baies:\n",
        "nb = Pipeline([('vect', CountVectorizer()),\n",
        "               ('clf', MultinomialNB()),\n",
        "              ])\n",
        "nb.fit(training_bbc['text'], training_bbc['category'])\n",
        "category_predict = nb.predict(test_bbc['text'])\n",
        "print(classification_report(test_bbc['category'], category_predict))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQR91FiQrax2"
      },
      "source": [
        "### 6. Baseline Classifier\n",
        "\n",
        "You can compare the performance of your Machine Learning model with a simple baseline classifier. One possibility could be to use a classifier that generates predictions by respecting the training setâ€™s class distribution. You can consider using [Dummy classifier](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html) from scikit learn.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "GZunaVeFrax3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.17953321364452424\n"
          ]
        }
      ],
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "# Evaluate the random baseline\n",
        "baseline = DummyClassifier(strategy=\"stratified\")\n",
        "\n",
        "# To do: Add the code below to train the baseline classifier and evaluate it.\n",
        "baseline.fit(training_bbc['text'],training_bbc['category'])\n",
        "category_predict=baseline.predict(test_bbc['text'])\n",
        "print(baseline.score(test_bbc['category'], category_predict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hoa-P1QIrax4"
      },
      "source": [
        "Is the result from the baseline classifier justified?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwzvJ6qMrax4"
      },
      "source": [
        "### 6. Grid Search\n",
        "\n",
        "So far, you have trained the vectorizer and the classifer using their default parameters. However, in practical settings, one needs to optimize the parameters of the model to maximize the performance. \n",
        "\n",
        "Here, you are asked to find the optimal parameters for the pipelines that you have created above using a 5 fold cross validation. The choice of hyperparameters for optimization are:\n",
        "\n",
        "1. Bi-grams vs uni-grams vs tri-grams from [Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). \n",
        "2. Additive smoothing  for the Multinomial naive bayes classifier $\\alpha$ = {1, 0.1}\n",
        "3. Tokenized vs non-tokenized text (For tokenization, you can use the function 'preprocess' that is given below as a parameter for the vectorizer.)\n",
        "\n",
        "\n",
        "You can refer to the [Grid search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) documentation from the scikit-learn library.\n",
        "\n",
        "Finally, print the parameters from the grid search that give the best performance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "fdhGS2HMU3Ac"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function preprocess can be used as a tokenizer.\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm', disable=[\"tagger\", \"parser\", \"ner\"])\n",
        "'''\n",
        "for handling error \"Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.\"\n",
        "we need to download \"python -m spacy download en_core_web_sm\" in cmd\n",
        "'''\n",
        "\n",
        "def preprocess(text):\n",
        "    doc = nlp(text)\n",
        " \n",
        "    final_key=[]\n",
        "    for token in doc:\n",
        "        if token.is_stop==False and token.lemma_.isalpha():\n",
        "            \n",
        "            final_key.append(token.lemma_)\n",
        "        \n",
        "    return final_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "nOz9OcIlrax4"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "__init__() missing 1 required positional argument: 'estimator'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_34732/2188446634.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m           }\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mgrd_sr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mgrd_sr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_bbc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_bbc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'category'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'estimator'"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# To do: Replace the ??? in the code and implement the grid search\n",
        "# Note: Take a look at how you an specify the parameters for grid search from an example of n-grams. Similarly, you can specify the other remaining parameters.\n",
        "params = {'vectorizer__ngram_range':[(1,1), (1,2), (1,3)],\n",
        "          'clf_alpha':(1, 0.1),\n",
        "          'vectorizer__preprocessor':(None, preprocess)          \n",
        "          }\n",
        "\n",
        "kf = KFold()\n",
        "grd_sr = GridSearchCV(estimator=kf ,param_grid=params, cv=5)\n",
        "grd_sr.fit(training_bbc['text'], training_bbc['category'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8ODDYpS626Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GISqsrKD8md0"
      },
      "source": [
        "## 7. Fine-tuning using BERT\n",
        "\n",
        "In this section, you will see how a pre-trained BERT model can be fine tuned for the task of text classification. \n",
        "\n",
        "Run the following cells to fine-tune the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-zjPdPiZLEG"
      },
      "outputs": [],
      "source": [
        "'Download the tokenizer and BERT module for python'\n",
        "\n",
        "#!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n",
        "\n",
        "\n",
        "\n",
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wks6VNo68xk-"
      },
      "outputs": [],
      "source": [
        "'Import all the necessary modules'\n",
        "\n",
        "#import tokenization\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import bert\n",
        "from tensorflow.keras.models import  Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"TensorFlow Version:\",tf.__version__)\n",
        "print(\"Hub version: \",hub.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCiSoIU28656"
      },
      "outputs": [],
      "source": [
        "'Download the pretrained BERT model'\n",
        "\n",
        "m_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
        "bert_layer = hub.KerasLayer(m_url, trainable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zK-Skk-K87oz"
      },
      "outputs": [],
      "source": [
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "#tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "\n",
        "'Use BERT tokenizer'\n",
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "tokenizer=FullTokenizer(vocab_file,do_lower_case)\n",
        "\n",
        "\n",
        "def bert_encode(texts, tokenizer, max_len=512):\n",
        "    all_tokens = []\n",
        "    all_masks = []\n",
        "    all_segments = []\n",
        "    \n",
        "    for text in texts:\n",
        "        text = tokenizer.tokenize(text)\n",
        "        \n",
        "        text = text[:max_len-2]\n",
        "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
        "        pad_len = max_len-len(input_sequence)\n",
        "        \n",
        "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n",
        "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
        "        segment_ids = [0] * max_len\n",
        "        \n",
        "        all_tokens.append(tokens)\n",
        "        all_masks.append(pad_masks)\n",
        "        all_segments.append(segment_ids)\n",
        "        \n",
        "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hF_gDNZn8-LP"
      },
      "outputs": [],
      "source": [
        "def build_model(bert_layer, max_len=512):\n",
        "    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
        "    \n",
        "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "    \n",
        "    clf_output = sequence_output[:, 0, :]\n",
        "    \n",
        "    lay = tf.keras.layers.Dense(64, activation='relu')(clf_output)\n",
        "    lay = tf.keras.layers.Dropout(0.2)(lay)\n",
        "    lay = tf.keras.layers.Dense(32, activation='relu')(lay)\n",
        "    lay = tf.keras.layers.Dropout(0.2)(lay)\n",
        "    out = tf.keras.layers.Dense(5, activation='softmax')(lay)\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
        "    model.compile(tf.keras.optimizers.Adam(lr=2e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eM05qXjS9Bbj"
      },
      "outputs": [],
      "source": [
        "'Set the maximum length of the sequence'\n",
        "max_len = 512\n",
        "\n",
        "'Transform non-numerical labels to numerical'\n",
        "label = preprocessing.LabelEncoder()\n",
        "train_labels = label.fit_transform(training_bbc['category'])\n",
        "train_labels = to_categorical(train_labels)\n",
        "\n",
        "'Prepare the input by tokenising and padding the text sequence'\n",
        "train_input = bert_encode(training_bbc.text.values, tokenizer, max_len=max_len)\n",
        "test_input = bert_encode(test_bbc.text.values, tokenizer, max_len=max_len)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0yJERe39F1F"
      },
      "outputs": [],
      "source": [
        "labels = label.classes_\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HudNWXyS9L5z"
      },
      "outputs": [],
      "source": [
        "'Build the model'\n",
        "\n",
        "model = build_model(bert_layer, max_len=max_len)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bxd085OY9Rn4"
      },
      "outputs": [],
      "source": [
        "'Start training the model'\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
        "\n",
        "train_sh = model.fit(\n",
        "    train_input, train_labels,\n",
        "    validation_split=0.1,\n",
        "    epochs=1,\n",
        "    callbacks=[checkpoint, earlystopping],\n",
        "    batch_size=4,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2Zr2lw1ClM7"
      },
      "outputs": [],
      "source": [
        "'Predict the classes from the fine-tuned BERT model'\n",
        "bert_pred = model.predict(test_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_E9ufij-DU2q"
      },
      "outputs": [],
      "source": [
        "'Invert the classes from numerical to non-numerical (original) categories'\n",
        "y_pred_bert = label.inverse_transform(np.argmax(bert_pred.round().astype(int), axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gnni25tiC-Ng"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(classification_report(test_bbc['category'], y_pred_bert, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyNKl7Z6_Sj5"
      },
      "source": [
        "1. Comment on the results. Is there any improvement in performance when compared to MultinomialNB?\n",
        "\n",
        "2. Try changing the number of epochs to 3 and then 5 to see if there is any improvement in the performance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLev7gYhF_D2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Text_classification_lab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
